# 第4章 决策树

参考课程链接：【【吃瓜教程】《机器学习公式详解》（南瓜书）与西瓜书公式推导直播合集】 https://www.bilibili.com/video/BV1Mh411e7VU/?p=7&share_source=copy_web&vd_source=c22abe8e67e193936015d5ca043a8148

# 4.1 基本流程

决策树（Decision Tree）是一种常用的机器学习算法，用于分类和回归任务。其基本思想是基于特征对数据进行递归分割，使得数据的不确定性或混乱度逐渐降低。其结构像一棵树，包含节点和边。

决策树的主要组件：

1. **根节点**：从数据集的所有数据开始的节点。
2. **分支或决策节点**：根据某一特征的取值进行数据分割的节点。
3. **叶节点**：不再进一步分割的节点，表示决策结果。

决策树的构建过程：

1. 选择一个特征进行测试。
2. 根据该特征的测试结果分割数据。
3. 对每一个子数据集递归地重复步骤1和2，直到满足停止条件（如深度限制、叶节点达到最小的样本数等）。

选择特征和分割点的准则：

为了选择最佳的特征和分割点，通常使用如下的评价标准：

1. **信息增益**：基于信息熵的减少来评估分割的好坏。
2. **信息增益比**：在信息增益的基础上，考虑了特征本身的信息量。
3. **基尼不纯度**：评估数据集的混乱度。
4. **均方误差**：用于回归任务的评价标准。

决策树的优点：

1. **易于理解和解释**：决策树的结构直观，可以直观地看到哪些特征重要、如何做出决策。
2. **需要较少的数据预处理**：不需要归一化、缺失值处理等。
3. **可以处理数值型和分类型数据**。

决策树的缺点：

1. **容易过拟合**：决策树有时可能会过度匹配数据，尤其是当树很深时。
2. **局部最优**：贪婪地从顶部到底部构建树可能不总是产生全局最优的决策树。
3. **不稳定**：数据的小变动可能导致完全不同的树。

为了解决过拟合问题，通常采用剪枝策略（如预剪枝和后剪枝）。此外，随机森林等集成方法也常用于提高决策树的性能和稳定性。

## 算法伪代码（生成决策树）

```
函数 GenerateDecisionTree(D, attributes):
    输入:
        D: 训练数据集
        attributes: 当前可用的特征集

    输出:
        决策树

    1. 如果 D 中所有实例都属于同一类别 C:
         返回单节点树 T，标记为类 C
         
    2. 如果 attributes 为空：
         返回单节点树 T，标记为 D 中最频繁的类
         
    3. 选择信息增益最大的特征 best_attribute
    4. 为 best_attribute 的每一个值 vi:
         4.1. 为 D 中 best_attribute = vi 的实例子集 Di:
                4.1.1. 如果 Di 为空：
                          将新的分支下的叶子节点标记为 D 中最频繁的类
                4.1.2. 否则递归调用 GenerateDecisionTree(Di, attributes - {best_attribute})
         
    5. 返回构建的决策树

```

这个伪代码描述了一个递归算法，通过不断地选择信息增益最大的特征来分割数据集，直到满足某些停止条件（例如所有数据属于同一类或已经没有可用的特征）为止。上述伪代码只是为了简化描述而给出的。在实际实现中，还需要考虑更多的细节，如缺失值处理、剪枝策略等。

# 4.2 划分选择

决策树学习中选择最优划分属性的目的是为了找到一个最好的特征，使得基于这个特征的分割可以让子数据集的纯度最大（对于分类问题）或者误差最小（对于回归问题）。

## 4.2.1 信息增益（Information Gain）

- 基于熵 (entropy) 的概念。
- 选择那些能够带来熵降低（即纯度增加）最多的特征。
- 公式: $Gain(D,f)=Entropy(D)-\sum_{v=1}^V\frac{|D_v|}{|D|}Entropy(D_v)$，其中 $D_v$ 是数据集 $D$ 在特征 $f$ 上取值为 $v$ 的子集。

## 4.2.2 增益率（Gain Ratio）

- 对信息增益的一个修正，通过考虑特征的固有信息量来消除特征取值多的偏好。
- 公式: $Gain\_Ratio(D,f)=\frac{Gain(D,f)}{IV(f)}$，其中 $IV(f)$ 是特征 $f$ 的固有值，$IV(f)=-\sum_{v=1}^V\frac{|D_V|}{|D|}\log_2\frac{|D_v|}{|D|}$。

## 4.2.3 基尼指数（Gini Index）

- 用于CART (Classification and Regression Trees) 算法。
- 测量一个随机选择的样本被错误分类的概率。
- 公式: $Gini(D)=1-\sum p_i^2$，其中 $p_i$ 是数据集 $D$ 中第 $i$ 类的概率。

- $Gini\_index(D,f)=\sum_{v=1}^V\frac{|D^v|}{|D|}Gini(D^v)$

CART使用Gini Index进行特征选择，并进行二叉分裂。这意味着，即使特征是多值的，CART也会找到最佳的二叉分裂点。对于连续值的特征，CART会考虑所有可能的分裂点并选择最佳的。

所以实际使用Gini_index时，是通过来$Gini\_index(D,f)=\frac{|D^{f=v}|}{|D|}Gini(D^{f=v})+\frac{|D^{f\neq v}|}{|D|}Gini(D^{f\neq v})$进行计算的。

# 4.3 剪枝处理

剪枝（pruning）是决策树学习算法中的一个重要步骤，用于去掉决策树中的一部分分支，从而避免过拟合。过拟合是机器学习模型在训练数据上表现得非常好，但在新的、未见过的数据上表现较差的情况。决策树特别容易出现过拟合，因为它们可以通过增加更多的分支来完美地适应训练数据。

剪枝可以在决策树生成的过程中（即预剪枝）或在决策树生成后（即后剪枝）进行。以下是预剪枝和后剪枝的简要介绍：

1. **预剪枝 (Pre-pruning)**
   - 在决策树的构建过程中进行。
   - 定义某些准则来提前停止树的增长，例如设定决策树的最大深度、节点中的最小样本数等。
   - 预剪枝可能会导致模型过于简化，即欠拟合。
2. **后剪枝 (Post-pruning)**
   - 在决策树构建完成后进行。
   - 通常需要删除子树，并用叶节点取而代之。
   - 方法通常包括：使用验证集来检验子树的性能，如果去掉某个子树能够在验证集上获得同样或更好的性能，就将其剪去。
   - 通常，后剪枝比预剪枝得到的决策树性能更好，但计算成本更高。

无论是预剪枝还是后剪枝，其核心思想都是牺牲训练数据上的一些准确性，以获取在测试或未知数据上更好的泛化性能。

在实践中，剪枝是提高决策树性能的有效手段。尤其是在决策树很深，或者训练数据量不大的情况下，剪枝尤为重要。

# 4.4 连续与缺失值

## 4.4.1 连续值处理

处理连续属性生成决策树的核心问题是如何确定连续属性的最佳分裂点。以下是处理连续属性生成决策树的一般步骤：

1. **对连续属性进行排序**:
   - 首先，对训练数据中的连续属性值进行排序。
2. **确定潜在的分裂点**:
   - 一种常见的策略是考虑每两个连续的值的中点作为潜在的分裂点。
   - 例如，如果连续值为 [2, 4, 6, 8]，则潜在的分裂点可以是 3, 5, 和 7。
3. **计算每个潜在分裂点的不纯度**:
   - 对于每个潜在的分裂点，将数据分为两部分：小于等于该值的部分和大于该值的部分。
   - 计算分裂后的不纯度。这可以使用信息增益、基尼不纯度或其他方法。
4. **选择最佳分裂点**:
   - 选择可以使不纯度最小（或信息增益最大）的分裂点作为该连续属性的最佳分裂点。
5. **递归分裂**:
   - 在选择了最佳分裂点后，数据集会被分成两个子集。然后，为每个子集递归地生成决策树。
6. **处理其他属性**:
   - 对于当前节点的数据集，除了最佳的连续属性，其他的离散属性或连续属性也需要考虑和比较，确定在当前节点上的最佳分裂属性。

需要注意的是，连续属性可以在多个节点上使用，每次可能有不同的分裂点。

## 4.4.2 缺失值处理

决策树在处理数据中的缺失值时有多种策略。这些策略可以分为两大类：处理训练数据中的缺失值和处理在预测过程中的缺失值。以下是一些常用方法：

1. **处理训练数据中的缺失值**:

   a. **跳过含有缺失值的数据**: - 当计算分裂准则（例如信息增益或基尼不纯度）时，可以只考虑完整的数据。

   b. **分配权重**: - 为每个可能的分裂值分配一个权重，这个权重与该值在没有缺失的数据中的分布相匹配。

   c. **多重分裂**: - 在具有缺失值的属性上考虑多种可能的分裂，然后选择最佳的分裂。

   d. **代理分裂**: - 当某个属性在数据中缺失时，选择第二最佳的属性进行分裂，这被称为“代理分裂”。

   e. **填充缺失值**: - 使用该属性的平均值、中位数或众数填充缺失值。

2. **处理预测过程中的缺失值**:

   a. **多路径遍历**: - 当在预测过程中遇到具有缺失属性的数据点时，可以将数据点同时发送到左右子树，并基于子树的预测结果加权决策。

   b. **使用最常见路径**: - 如果决策树在构建时使用了一个属性，并且在预测时该属性的值缺失，可以选择最常见的路径（例如，基于训练数据中更常见的分支）。

   c. **使用代理分裂**: - 如果主要分裂属性的值缺失，可以使用在训练过程中确定的代理分裂属性。

需要注意的是，不同的决策树实现可能会使用上述方法的不同组合。例如，CART (Classification and Regression Trees) 方法经常使用代理分裂来处理缺失值。处理缺失值的最佳策略可能会根据数据的特性和缺失值的分布模式而变化，因此在实践中可能需要尝试多种方法。

## 补充：常规缺失值处理方式（非决策树实现）

1. **删除含有缺失值的记录**:
   - 这是最简单的方法。如果数据点（行）中包含缺失值，直接删除该数据点。
   - 优点：操作简单，无需估计缺失值。
   - 缺点：可能会丢失大量数据，尤其在存在大量缺失值的数据集中。
2. **填充缺失值**:
   - **使用固定值**：如填充0、-999或其他指定值。
   - **使用属性的平均值、中位数或众数**：对于连续属性，通常使用平均值或中位数；对于离散属性，使用众数。
   - **使用插值方法**：如线性插值、多项式插值或基于时间序列的插值。
   - **使用模型预测**：如使用其他属性建立一个回归、分类或其他模型，预测缺失值。
3. **使用填充指示器**:
   - 除了填充缺失值，还可以为数据集添加一个新的二进制属性，表示原始属性是否缺失。这可以为模型提供关于值是否缺失的附加信息。
4. **基于算法的处理**:
   - 一些算法可以自然地处理缺失值，无需显式的预处理。例如，在构建决策树时，可以根据没有缺失值的数据选择分裂属性和分裂点，并在遇到缺失值时将数据同时发送到左右子树。
5. **使用多重插补**:
   - 这是一种更复杂的方法，其中缺失值不是被单一的估计替代，而是通过多次插补生成多个完整的数据集。然后可以在这些数据集上分别训练模型，并将结果进行合并。这种方法可以考虑缺失值的不确定性。

处理缺失值的最佳方法取决于数据的性质、缺失值的数量、缺失机制（例如是否完全随机缺失）以及用于分析的具体模型或算法。在进行任何处理之前，理解为什么数据可能会缺失以及缺失值的分布模式通常很有帮助。

# 4.5 多变量决策树

传统的决策树（例如CART、ID3、C4.5等）生成的分类边界是与属性维度坐标平行的，这意味着每一个决策树的节点都是基于单一属性的阈值进行数据分裂。这样的决策树很容易理解，但可能不是非常灵活，特别是当真实的分类边界是对角线或复杂形状时。

为了生成更复杂的分类边界，可以考虑使用多变量的线性组合作为节点。这意味着在每个节点，不是基于一个属性的值进行分裂，而是基于多个属性线性组合的值进行分裂。具体来说，决策可以基于以下形式的线性不等式：

$a_1x_1+a_2x_2+...+a_nx_n\le b$

其中，$x_1,x_2,...,x_n$是数据点的属性，$a_1,a_2,...,a_n$ 是这些属性的权重，而$b$是一个常数。

此类方法可以生成复杂的、不与坐标轴平行的分类边界，从而可能更好地适应某些数据分布。然而，这也使得模型更难解释，并可能增加过拟合的风险。

为了实现此类决策树，需要更复杂的算法来确定最佳的属性权重和阈值。在实践中，这种方法并不常见，部分原因是其计算复杂性较高，而且模型的解释性较差。

对于需要复杂分类边界的情况，其他的机器学习模型（如支持向量机、神经网络）可能更为适合。不过，如果你希望结合决策树的可解释性和复杂分类边界的能力，可以考虑使用随机森林或梯度提升树，它们通过集成多个简单的决策树来生成复杂的分类边界。

### 随机森林 (Random Forest)

1. 基本概念:
   - 随机森林是由多个决策树组成的集成学习方法。
2. 训练方法:
   - 对于每棵树，从训练数据中通过**有放回**的抽样（即自助采样，Bootstrap Sampling）抽取一个子集。
   - 在每个节点上，随机选择一定数量的属性，并从中选择最佳的属性进行分裂。这增加了模型的随机性。
3. 预测方法:
   - 对于分类问题，森林中的每棵树都会产生一个预测类别，最终的预测结果是这些预测中的多数投票结果。
   - 对于回归问题，预测的结果是所有树预测结果的平均值。
4. 优点:
   - 通过集成多个树，随机森林能够降低过拟合，提高泛化能力。
   - 并行化容易，因为每棵树都是独立训练的。
   - 自然地能够处理缺失值。
   - 可以输出属性的重要性评分。

### 梯度提升树 (Gradient Boosted Trees)

1. 基本概念:
   - 梯度提升树也是一个集成方法，但与随机森林不同，它是通过**逐步添加**决策树来优化模型的。
2. 训练方法:
   - 初始阶段，所有的数据点都有相同的权重。
   - 训练第一棵树来拟合当前的残差（例如，与平均值的差异）。
   - 基于第一棵树的预测误差，更新数据点的权重。
   - 使用更新后的权重训练下一棵树。
   - 重复此过程，直到达到预定的树的数量或误差收敛。
3. 预测方法:
   - 所有树的预测结果都被累加起来，得到最终的预测结果。
4. 优点:
   - 通常能够达到非常高的预测精度。
   - 可以自然地处理各种类型的数据：连续值、离散值以及缺失值。
   - 提供属性的重要性评分。
5. 注意事项:
   - 梯度提升树需要更多的调参，如学习速率、树的深度等。
   - 虽然能够达到很高的准确率，但是也更容易过拟合，特别是当树的数量很多时。