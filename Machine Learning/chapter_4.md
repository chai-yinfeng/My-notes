# 第4章 决策树

# 4.1 基本流程

决策树（Decision Tree）是一种常用的机器学习算法，用于分类和回归任务。其基本思想是基于特征对数据进行递归分割，使得数据的不确定性或混乱度逐渐降低。其结构像一棵树，包含节点和边。

决策树的主要组件：

1. **根节点**：从数据集的所有数据开始的节点。
2. **分支或决策节点**：根据某一特征的取值进行数据分割的节点。
3. **叶节点**：不再进一步分割的节点，表示决策结果。

决策树的构建过程：

1. 选择一个特征进行测试。
2. 根据该特征的测试结果分割数据。
3. 对每一个子数据集递归地重复步骤1和2，直到满足停止条件（如深度限制、叶节点达到最小的样本数等）。

选择特征和分割点的准则：

为了选择最佳的特征和分割点，通常使用如下的评价标准：

1. **信息增益**：基于信息熵的减少来评估分割的好坏。
2. **信息增益比**：在信息增益的基础上，考虑了特征本身的信息量。
3. **基尼不纯度**：评估数据集的混乱度。
4. **均方误差**：用于回归任务的评价标准。

决策树的优点：

1. **易于理解和解释**：决策树的结构直观，可以直观地看到哪些特征重要、如何做出决策。
2. **需要较少的数据预处理**：不需要归一化、缺失值处理等。
3. **可以处理数值型和分类型数据**。

决策树的缺点：

1. **容易过拟合**：决策树有时可能会过度匹配数据，尤其是当树很深时。
2. **局部最优**：贪婪地从顶部到底部构建树可能不总是产生全局最优的决策树。
3. **不稳定**：数据的小变动可能导致完全不同的树。

为了解决过拟合问题，通常采用剪枝策略（如预剪枝和后剪枝）。此外，随机森林等集成方法也常用于提高决策树的性能和稳定性。

## 算法伪代码（生成决策树）

```
函数 GenerateDecisionTree(D, attributes):
    输入:
        D: 训练数据集
        attributes: 当前可用的特征集

    输出:
        决策树

    1. 如果 D 中所有实例都属于同一类别 C:
         返回单节点树 T，标记为类 C
         
    2. 如果 attributes 为空：
         返回单节点树 T，标记为 D 中最频繁的类
         
    3. 选择信息增益最大的特征 best_attribute
    4. 为 best_attribute 的每一个值 vi:
         4.1. 为 D 中 best_attribute = vi 的实例子集 Di:
                4.1.1. 如果 Di 为空：
                          将新的分支下的叶子节点标记为 D 中最频繁的类
                4.1.2. 否则递归调用 GenerateDecisionTree(Di, attributes - {best_attribute})
         
    5. 返回构建的决策树

```

这个伪代码描述了一个递归算法，通过不断地选择信息增益最大的特征来分割数据集，直到满足某些停止条件（例如所有数据属于同一类或已经没有可用的特征）为止。上述伪代码只是为了简化描述而给出的。在实际实现中，还需要考虑更多的细节，如缺失值处理、剪枝策略等。

# 4.2 划分选择

决策树学习中选择最优划分属性的目的是为了找到一个最好的特征，使得基于这个特征的分割可以让子数据集的纯度最大（对于分类问题）或者误差最小（对于回归问题）。

## 4.2.1 信息增益（Information Gain）

- 基于熵 (entropy) 的概念。
- 选择那些能够带来熵降低（即纯度增加）最多的特征。
- 公式: $Gain(D,f)=Entropy(D)-\sum_{v=1}^V\frac{|D_v|}{|D|}Entropy(D_v)$，其中 $D_v$ 是数据集 $D$ 在特征 $f$ 上取值为 $v$ 的子集。

## 4.2.2 增益率（Gain Ratio）

- 对信息增益的一个修正，通过考虑特征的固有信息量来消除特征取值多的偏好。
- 公式: $Gain\_Ratio(D,f)=\frac{Gain(D,f)}{IV(f)}$，其中 $IV(f)$ 是特征 $f$ 的固有值，$IV(f)=-\sum_{v=1}^V\frac{|D_V|}{|D|}\log_2\frac{|D_v|}{|D|}$。

## 4.2.3 基尼指数（Gini Index）

- 用于CART (Classification and Regression Trees) 算法。
- 测量一个随机选择的样本被错误分类的概率。
- 公式: $Gini(D)=1-\sum p_i^2$，其中 $p_i$ 是数据集 $D$ 中第 $i$ 类的概率。

- $Gini\_index(D,f)=\sum_{v=1}^V\frac{|D^v|}{|D|}Gini(D^v)$

CART使用Gini Index进行特征选择，并进行二叉分裂。这意味着，即使特征是多值的，CART也会找到最佳的二叉分裂点。对于连续值的特征，CART会考虑所有可能的分裂点并选择最佳的。

所以实际使用Gini_index时，是通过来$Gini\_index(D,f)=\frac{|D^{f=v}|}{|D|}Gini(D^{f=v})+\frac{|D^{f\neq v}|}{|D|}Gini(D^{f\neq v})$进行计算的。

# 4.3 剪枝处理



# 4.4 连续与缺失值



# 4.5 多变量决策树