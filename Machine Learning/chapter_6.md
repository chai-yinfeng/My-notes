# 第6章 支持向量机

参考课程链接：【【吃瓜教程】《机器学习公式详解》（南瓜书）与西瓜书公式推导直播合集】 https://www.bilibili.com/video/BV1Mh411e7VU/?p=9&share_source=copy_web&vd_source=c22abe8e67e193936015d5ca043a8148

支持向量机（Support Vector Machine, SVM）是一种在分类和回归分析中应用的监督学习模型。它是由Vapnik等人在1990年代初提出的，是统计学习理论中的重要研究内容。

支持向量机的主要思想是找到一个超平面（在高维空间中可能是超平面，在二维空间中可能是一条直线），该超平面可以将不同类别的数据点分隔开来。这个超平面被选择的原因是因为它具有最大的间隔（或称为“街道”），这意味着它距离最近的两类数据点的距离最大。

以下是支持向量机的一些关键概念：

1. **线性可分**：在特定的特征空间中，如果两类数据可以被一个直线（在二维）或超平面（在更高的维度）完美地分开，那么这两类数据被认为是线性可分的。
2. **最大间隔超平面**：SVM试图找到这样一个超平面，使得该超平面与每一类中最近的数据点的距离（也称为间隔）最大。
3. **支持向量**：离超平面最近并且决定超平面位置的数据点被称为支持向量。
4. **核函数**：对于非线性可分的数据，SVM可以通过使用核函数将数据映射到一个高维空间，使得在这个新空间中数据变得线性可分。常见的核函数有：线性核、多项式核、径向基核（RBF）等。
5. **软间隔**：在实际应用中，数据可能由于噪音或其他原因而不是完全线性可分的。为了处理这种情况，SVM引入了所谓的软间隔概念，允许某些数据点可以被错误分类。

支持向量机在很多应用中都表现得很好，尤其是在高维数据的分类任务中。然而，对于大数据集，SVM可能需要较长的训练时间，并且选择合适的核函数和参数也是一个挑战。

# 6.1 间隔与支持向量

## 最大间隔超平面

当我们讨论两类分类问题时，假设这两类数据在特征空间中是线性可分的，那么存在无数个超平面可以将这两类数据分开。但支持向量机（SVM）的核心思想是找到一个特定的超平面，它不仅可以分隔两类数据，而且距离两类数据的最近点的距离最远。这就是所谓的“最大间隔”策略。

这个间隔是从超平面到最近的正类数据点和到最近的负类数据点的距离的总和。最大间隔超平面的目的是最大化这个间隔，以确保对新的、未知的数据具有良好的泛化能力。

对给定训练样本集$D=\{(\boldsymbol x_1,y_1),(\boldsymbol x_2,y_2),...,(\boldsymbol x_m,y_m)\},y\in\{-1,+1\}$，划分超平面可通过如下线性方程描述：$\boldsymbol {w^Tx}+b=0$，其中$\boldsymbol w=(w_1;w_2;...;w_d)$为法向量，决定超平面的方向；$b$为位移项，决定超平面与远点之间的距离。因此，这个划分超平面可以被法向量$\boldsymbol w$和位移$b$唯一确定，记为$(\boldsymbol w,b)$。样本空间中任意一点$\boldsymbol x$到超平面$(\boldsymbol w,b)$的距离可写为：
$$
r=\frac{|\boldsymbol {w^Tx}+b|}{||\boldsymbol w||}
$$

## 几何间隔

几何间隔是一个重要概念，它帮助我们量化一个数据点到分类超平面的“距离”。在支持向量机（SVM）的背景下，我们尤其关心正负样本到分类超平面的距离，因为SVM试图最大化这些距离，以找到最优的分类超平面。

沿用上述内容对超平面的描述和表示：$\boldsymbol {w^Tx}+b=0$，其中$\boldsymbol w=(w_1;w_2;...;w_d)$为法向量；$b$为位移项

对于一个给定的样本点$(\boldsymbol x_i,y_i),y\in\{-1.+1\},i=1,2,...,m$，几何间隔 $\gamma$ 定义为：
$$
\gamma_i=\frac{y_i(\boldsymbol {w^Tx_i}+b)}{||\boldsymbol w||}
$$


注意这里$||\boldsymbol w||$是单位法向量，所以几何间隔实际上是点到超平面的真实距离。

对于给定数据集$X$和超平面$\boldsymbol {w^Tx}+b=0$，定义数据集$X$关于超平面的几何间隔为：数据集$X$中所有样本点的几何间隔最小值：
$$
\gamma=\min_{i=1,2,...,m}\gamma
$$
给定线性可分数据集$X$，支持向量机模型希望求得数据集$X$关于超平面的几何间隔$\gamma$达到最大的那个超平面，然后套上一个sign函数实现分类功能：
$$
y=sign(\boldsymbol {w^Tx}+b)=\left\{\begin{matrix}
1 & \boldsymbol {w^Tx}+b>0\\
-1 & \boldsymbol {w^Tx}+b<0
\end{matrix}\right.
$$

## 支持向量

当我们找到了这个最大间隔超平面，距离该超平面最近的数据点被称为“支持向量”。这些点的位置决定了超平面的位置和方向。换句话说，如果我们移除其他所有点并仅保留这些支持向量，超平面的位置和方向将保持不变。

从数学的角度看，这些支持向量是优化问题的约束条件，它们满足等式约束，即它们距离超平面的距离正好等于间隔。

为什么这些点叫做“支持向量”呢？因为这些向量“支持”了超平面的构建。如果没有这些向量，超平面的位置就会改变。



给定线性可分数据集$X$，设$X$中几何间隔最小的样本为$(\boldsymbol x_{min},y_{min})$，那么支持向量机找超平面的过程可以转化为以下带约束条件的优化问题：
$$
\max_{\boldsymbol w,b}\frac{y_{min}(\boldsymbol {w^Tx_{min}}+b)}{||\boldsymbol w||}\\
s.t.\frac{y_i(\boldsymbol {w^Tx_i}+b)}{||\boldsymbol w||}\ge\frac{y_{min}(\boldsymbol {w^Tx_{min}}+b)}{||\boldsymbol w||},i=1,2,...,m
$$
假设该问题的最优解为$(\boldsymbol w^*,b^*)$，那么$(\alpha\boldsymbol w^*,\alpha b^*),\alpha \in \mathbb R^+ $也是最优解，且超平面也不变，因此还需要对 $\boldsymbol w,b$ 做一定限制才能使得上述优化问题有可解的唯一解。不妨令$y_{min}(\boldsymbol {w^Tx_{min}}+b)=1$，因为对于特定的$(\boldsymbol x_{min},y_{min})$来说，能使得$y_{min}(\boldsymbol {w^Tx_{min}}+b)=1$的$\alpha$有且仅有一个。因此上述优化问题进一步转化为：
$$
\max_{\boldsymbol w,b}\frac{1}{||\boldsymbol w||}\\
s.t.y_i(\boldsymbol {w^Tx_i}+b)\ge1,i=1,2,...,m
$$
考虑到最大化间隔仅需要最大化$||\boldsymbol w||^{-1}$，等价于最小化$||\boldsymbol w||^2$，改写上式就可以得到西瓜书中6.6式。
$$
\max_{\boldsymbol w,b}\frac{1}{2}||\boldsymbol w||^2\\
s.t.y_i(\boldsymbol {w^Tx_i}+b)\ge1,i=1,2,...,m
$$

# 6.2 对偶问题

上述问题的求解是一个凸二次规划，但是涉及到对输入数据集每个维度的求解，在面临输入参数维度较高的情况时会消耗更多资源，可以使用拉格朗日乘子法得到其“对偶问题”。

## 拉格朗日乘子法

拉格朗日乘子法是一种寻找多变量函数在一组约束下的极值（最大值或最小值）的方法。这种方法非常适用于优化问题，特别是当存在约束条件时。

考虑以下问题：我们希望找到函数 $f(x,y)$ 的极值，但是 $x$ 和 $y$ 必须满足某个约束，例如 $g(x,y)=0$。

拉格朗日乘子法的基本思想是将这个约束问题转化为无约束问题。为此，我们引入一个新的变量，称为拉格朗日乘子（通常用 $\lambda$ 表示），然后定义一个新的函数，称为拉格朗日函数：
$$
L(x,y,\lambda)=f(x,y)+\lambda g(x,y)
$$
然后，我们可以寻找 $L$ 的极值而不需要考虑约束。这是因为当 $g(x,y)=0$ 时，$L$ 和 $f$ 在某点的值是相同的，且 $L$ 在这点的梯度也是0。

为了找到 $f$ 的极值，我们需要求 $L$ 对 $x$、$y$ 和 $\lambda$ 的偏导数，并将它们设为0：
$$
\frac{\partial L}{\partial x}=0\\
\frac{\partial L}{\partial y}=0\\
\frac{\partial L}{\partial \lambda}=0
$$
上述方程组给出了 $f$ 在约束 $g(x,y)=0$ 下可能的极值点。

此方法也可以扩展到更高维度和更复杂的约束情况。



则该问题的拉格朗日函数可写为：
$$
L(\boldsymbol w,b,\boldsymbol \alpha)=\frac{1}{2}||\boldsymbol w||^2+\sum_{i=1}^m\alpha_i(1-y_i(\boldsymbol {w^Tx_i}+b))
$$
其中$\boldsymbol \alpha=(\alpha_1;\alpha_2;...;\alpha_m)$，令$L(\boldsymbol w,b,\boldsymbol \alpha)$对$\boldsymbol w$和$b$的偏导为零，并代入上式，可以得到对偶问题：
$$
\max_\boldsymbol \alpha \sum_{i=1}^m\alpha_i-\frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m\alpha_i\alpha_jy_iy_j\boldsymbol {x_i^Tx_j}\\
s.t.\sum_{i=1}^m\alpha_iy_i=0,\\
\alpha_i\ge0,i=1,2,...,m
$$
最后可以解出$\boldsymbol \alpha$，求出$\boldsymbol w$和$b$即可得到模型
$$
\begin{align}
f(\boldsymbol x)&=\boldsymbol {w^Tx}+b\\
&=\sum_{i=1}^m\alpha_iy_i\boldsymbol {x_i^Tx}+b
\end{align}
$$
关于求解对偶问题，可以使用通用的二次规划算法求解，也可以使用SMO算法。

## SMO算法

SMO（Sequential Minimal Optimization）算法是一个用于求解支持向量机（SVM）中二次规划问题的算法。该算法是由John Platt于1998年在微软研究院提出的。SMO算法的主要目的是为了提高训练SVM的效率。

SVM的训练过程可以转化为一个带约束的二次规划问题。对于这样的问题，直接求解会非常计算密集，特别是在数据点数量很大的情况下。SMO算法提出了一种高效的方法，通过一次只优化两个拉格朗日乘子来解决这个问题。

SMO算法的主要思想如下：

1. **选择两个拉格朗日乘子**：首先选择一个违反KKT条件最严重的乘子（通常称为外层乘子）。然后选择另一个乘子，使得这两个乘子组合起来的优化问题最优（通常称为内层乘子）。
2. **优化这两个乘子**：在其他所有乘子固定的情况下，针对这两个乘子优化目标函数。
3. **更新阈值**：根据新的乘子值计算并更新SVM的阈值。
4. **重复步骤**：重复上述步骤，直到所有的拉格朗日乘子都满足KKT条件，或者达到预定的迭代次数。

为什么要一次选择两个乘子呢？这是因为我们有一个约束条件：所有的拉格朗日乘子与其对应的类标签的乘积之和必须为零。因此，当我们更改一个乘子时，为了满足这个约束，必须更改另一个乘子。

SMO算法的优点是每一步的计算都很简单，并且它可以确保快速收敛。这使得SMO成为SVM训练的流行方法，尤其是在中等大小的数据集上。

具体的数学迭代过程可以参照西瓜书即南瓜书。

# 6.3 核函数

核函数（Kernel Function）是在支持向量机（SVM）和其他机器学习算法中用于处理非线性数据的技术。核函数的主要思想是将数据从一个低维的原始空间映射到一个更高维的特征空间，使得数据在这个新的特征空间中变得线性可分或者更易于线性分隔。

在SVM中，我们通常希望找到一个超平面，使得两个不同的类别可以被这个超平面分开。但在某些情况下，数据在原始空间中是非线性的，并不能通过一个直线或超平面来分隔。通过使用核函数，我们可以隐式地将这些数据点映射到一个高维空间，而无需显式地计算映射后的坐标。这种技巧被称为"核技巧"。

以下是一些常见的核函数：

1. **线性核函数**: $K(x,z)=x^Tz$ 它表示原始空间中的点积，对应于线性SVM。
2. **多项式核函数**: $K(x,z)=(1+x^Tz)^d$ 其中，$d$ 是一个预先定义的非负整数，表示多项式的阶数。
3. **径向基函数（Radial Basis Function, RBF或高斯核函数）**: $K(z,x)=\exp(-\gamma||x-z||^2)$ 其中，$\gamma$ 是一个预先定义的正参数。
4. **Sigmoid核函数**: $K(z,x)=\tanh(\kappa x^Tz+c)$ 其中，$\kappa$ 和 $c$ 是预设参数。

核函数必须满足一些性质，如对称性和正定性。其基本要求是它所对应的内积在映射后的特征空间中应该是有效的。核技巧的一个巨大优势是它允许我们在高维空间中进行计算，而不需要显式地处理高维空间中的数据点，这极大地提高了计算效率。

在实际应用中，选择哪种核函数以及其参数（如RBF核的$\gamma$）对模型的性能有很大的影响，通常需要根据特定的任务和数据进行调整。

参考课程链接：【【吃瓜教程】《机器学习公式详解》（南瓜书）与西瓜书公式推导直播合集】 https://www.bilibili.com/video/BV1Mh411e7VU/?p=10&share_source=copy_web&vd_source=c22abe8e67e193936015d5ca043a8148

# 6.4 软间隔与正则化

## 软间隔

在支持向量机（SVM）中，我们提到的“间隔”是指数据点到分类超平面的距离。当数据是线性可分的，我们可以找到一个超平面使得所有数据点都正确分类，并且间隔最大化。这被称为“硬间隔”。

但在很多实际情况中，数据是线性不可分的，即不存在一个超平面能够完美地分隔所有的数据点。这可能是因为数据本身的噪音、异常值或者数据的非线性特性。在这种情况下，如果我们依然强调完美分隔，可能会导致模型过拟合。

为了处理这种情况，SVM引入了“软间隔”的概念。软间隔允许某些数据点被错误分类或位于超平面的错误一侧。为了实现这个目标，SVM引入了“松弛变量”（slack variables）。

给定一个数据点，其松弛变量可以是以下几种情况：

1. 等于0：表示数据点被正确分类，而且它位于超平面的正确一侧，距离超平面的距离至少是间隔的大小。
2. 介于0和1之间：表示数据点被正确分类，但它距离超平面的距离小于间隔。
3. 大于1：表示数据点被错误分类。

在软间隔SVM中，我们的目标不仅是最大化间隔，还要尽量减少这些松弛变量。为了平衡这两个目标，SVM引入了一个惩罚参数C，它定义了间隔最大化和误分类之间的权重。较大的C值会更强调误分类的惩罚，而较小的C值则允许更大的间隔，即使这意味着一些误分类。

总的来说，软间隔是SVM中处理线性不可分数据的方法，通过引入松弛变量和惩罚参数C来平衡间隔最大化和误分类的权重。

## 公式推导

从数学角度来说，软间隔就是允许部分样本（但要尽可能少）不满足下式中的约束条件
$$
\max_{\boldsymbol w,b}\frac{1}{2}||\boldsymbol w||^2\\
s.t.y_i(\boldsymbol {w^Tx_i}+b)\ge1,i=1,2,...,m
$$
因此，可以将必须严格执行的约束条件转化为具有一定灵活性的“损失” ，合格的损失函数要求如下：

- 当满足约束条件时，损失为0
- 当不满足约束条件时，损失不为0
- （可选）当不满足约束条件时，损失与其违反约束条件的程度成正比

只有满足以上要求，才能保证在最小化（min）损失的过程中，保证不满足约束条件的样本尽可能的少。

由以上条件可以写出优化目标：
$$
\min_{\boldsymbol w,b}\frac{1}{2}||\boldsymbol w||^2+C\sum_{i=1}^ml_{0/1}(y_i(\boldsymbol {w^Tx_i}+b)-1)
$$
其中$l_{0/1}$为“0/1损失函数”，$C>0$是一个常数。
$$
l_{0/1}(z)=\left\{\begin{matrix}
1, & if\ z<0\\
0, & otherwise
\end{matrix}\right.
$$
显然，当$C$为无穷大时，上式迫使所有样本均满足约束，于是等价于“硬间隔”的情况；当$C$取有限值时，允许一些样本不满足约束。

## 常见的损失函数

考虑到“0/1损失函数”非凸、非连续，数学性质不太好，用该损失函数会导致优化问题难以直接求解，所以可以用其他损失函数替代它。以下是软间隔支持向量机中常见的几种损失函数：

1. **合页损失（Hinge Loss）**:
   - 公式: $\max(0,1-y_i(w_ix_i+b))$
   - 这是标准的支持向量机使用的损失函数。当一个数据点被正确分类且距离超平面的距离大于或等于1时，其损失为0。当数据点位于间隔内或被错误分类时，损失则增加。
2. **平滑合页损失（Smoothed Hinge Loss）**:
   - 它是合页损失的平滑版本，适合于梯度下降等优化算法。
3. **对数损失（Logistic Loss）**:
   - 公式: $\log(1+exp(-y_i(w_ix_i+b)))$
   - 对数损失来源于逻辑回归，但也可以用于支持向量机。它给出了一种平滑的损失，适合于梯度下降优化。
4. **指数损失（Exponential Loss）**:
   - 公式: $\exp(-y_i(w_ix_i+b))$
   - 这种损失函数在Adaboost算法中常用，也可以用于支持向量机。
5. **平方合页损失（Squared Hinge Loss）**:
   - 公式: $\max(0,1-y_i(w_ix_i+b))^2$
   - 这是合页损失的平方版本，它更重视那些距离超平面很远的误分类点。

## 松弛变量

**松弛变量（Slack Variables）**在支持向量机（SVM）中被引入，主要是为了处理线性不可分的数据。在标准的硬间隔SVM中，我们试图找到一个超平面来完美地分隔两个类别，但在很多实际应用中，这样的完美分隔是不可能的，因为数据可能由于噪声、异常值或其他原因而变得线性不可分。

这就引出了软间隔SVM的概念，它允许某些数据点被错误地分类或者距离分类边界太近。为了量化这种“违规”，我们引入了松弛变量。

具体来说，对于每个数据点，我们都有一个对应的松弛变量。这些松弛变量的值反映了数据点到正确分类边界的距离。考虑以下几种情况：

1. 如果数据点被正确分类，并且距离分隔超平面的距离至少为间隔的大小，那么松弛变量的值为0。
2. 如果数据点被正确分类，但它距离分隔超平面的距离小于间隔，那么松弛变量的值在0和1之间。
3. 如果数据点被错误分类，那么松弛变量的值大于1。

引入松弛变量的一个原因是，它们允许我们形式化软间隔SVM的优化问题。具体来说，我们的目标不再是仅仅最大化间隔，而是最大化间隔的同时，尽量减少松弛变量的总值。为了平衡这两个目标，我们引入了一个惩罚参数$C$，它决定了我们对松弛变量的“容忍度”。

采用hinge损失，可以将优化问题写成：
$$
\min_{\boldsymbol w,b}\frac{1}{2}||\boldsymbol w||^2+C\sum_{i=1}^m\max(0,1-y_i(\boldsymbol {w^Tx_i}+b))
$$
引入“松弛变量”$\xi_i\ge0$，可将上式重新写为：
$$
\min_{\boldsymbol w,b,\xi_i}\frac{1}{2}||\boldsymbol w||^2+C\sum_{i=1}^m\xi_i\\
s.t.\ y_i(\boldsymbol {w^Tx_i}+b)\ge1-\xi_i\\
\xi_i\ge0,i=1,2,...,m
$$

## “结构风险”和“经验风险”

将$\min_{\boldsymbol w,b}\frac{1}{2}||\boldsymbol w||^2+C\sum_{i=1}^ml_{0/1}(y_i(\boldsymbol {w^Tx_i}+b)-1)$式中的0/1损失函数换成别的替代损失函数可以得到其他学习模型，这些模型的性质与所用的替代函数直接相关，但他们具有一个共性：优化目标中的第一项用来描述划分超平面的“间隔”大小，另一项用来表述训练集上的误差，可写为更一般的形式：
$$
\min_{f}\Omega(f)+C\sum_{i=1}^ml(y_i(f(\boldsymbol x_i),y)
$$
其中$\Omega(f)$称为“结构风险”，用于描述模型$f$的某些性质；第二项称为“经验风险”，用于描述模型与训练数据的期和成都；$C$用于对二者进行折中。从经验风险最小化的角度来看，$\Omega(f)$表述了我们希望获得具有何种性质的模型（例如希望获得复杂度较小的模型），这为引入领域知识和用户意图提供了途径；另一方面，该信息有助于削减假设空间，从而降低了最小化训练误差的过拟合风险。

### 经验风险（Empirical Risk）

经验风险是指模型在训练数据上的平均损失。它可以被定义为：$R_{emp}(f)=\frac{1}{N}\sum_{i=1}^NL(y_i,f(x_i))$ 其中，$N$ 是训练样本的数量，$L$ 是损失函数，$y_i$ 是第 $i$ 个样本的真实标签，而 $f(x_i)$ 是模型对第 $i$ 个样本的预测。简而言之，经验风险度量的是模型在训练数据上的性能。

### 结构风险（Structural Risk）

结构风险是在经验风险的基础上，加入了一个正则化项或复杂度惩罚。它的目的是防止模型过拟合，考虑模型的复杂性。结构风险可以被定义为：$R_{str}(f)=R_{emp}(f)+\lambda\Omega(f)$ 其中，$\Omega(f)$ 是模型复杂度或正则化项，而 $\lambda$ 是一个超参数，决定了正则化的强度。结构风险旨在在训练数据的拟合与模型的复杂度之间找到平衡，从而避免过拟合并提高模型的泛化能力。

从直观上理解，**经验风险**关心的是模型在训练数据上的性能，而**结构风险**关心的是模型的泛化能力，即在未知数据上的性能。通过最小化结构风险，我们试图在拟合训练数据和保持模型简单性之间找到一个平衡，从而获得良好的泛化性能。

## ”正则化“问题

正则化是机器学习和统计建模中常用的一种技术，其目的是防止模型过拟合，提高模型的泛化能力。过拟合是指模型在训练数据上表现得过于优秀，但在新的、未见过的数据上表现较差。这通常是因为模型太复杂，捕捉到了训练数据中的噪声而不是真正的信号。

**正则化的基本思想**是在模型的优化过程中，除了最小化损失函数（如均方误差），还要加上一个描述模型复杂度的惩罚项。这使得模型在拟合数据时更为“谨慎”，从而避免过拟合。

常见的正则化技术有：

1. **L1正则化（Lasso）**: $J(\theta)=Loss(\theta)+\lambda\sum_i|\theta_i|$ 其中，$\theta$ 是模型参数，$\lambda$ 是正则化系数。L1正则化会导致模型的某些参数为零，从而实现特征选择。
2. **L2正则化（Ridge）**: $J(\theta)=Loss(\theta)+\lambda\sum_i\theta_i^2$ L2正则化会使模型的所有参数都接近零，但不会使其完全为零。这有助于防止参数值过大，从而防止过拟合。
3. **Elastic Net**: 是L1和L2正则化的组合，同时具有L1和L2的特点。

正则化的关键超参数是$\lambda$（也有的文献中使用$\alpha$），它控制正则化的强度。较大的$\lambda$值会导致更强的正则化效果，可能导致模型欠拟合；而较小的$\lambda$值会导致较弱的正则化，可能导致过拟合。

**为什么正则化有效？**

直观地说，正则化通过对模型复杂度的惩罚来限制模型的自由度。在有限的数据中，高度复杂的模型可以很容易地记住每个数据点，但这并不意味着它能够很好地推广到新的数据点。通过对模型复杂度进行惩罚，正则化迫使模型集中注意力于数据中最重要的、最有代表性的模式，而忽略噪声和不重要的模式。

总的来说，正则化是一种用于提高模型泛化能力、防止过拟合的技术。

# 6.5 支持向量回归

支持向量回归（Support Vector Regression, SVR）是支持向量机（SVM）在回归问题上的应用。与支持向量机用于分类任务略有不同，支持向量回归的目标是预测一个连续值而不是一个类标签。

**基本思想：**

在支持向量分类中，我们试图找到一个超平面，使得两个类别之间的间隔最大化。而在支持向量回归中，我们的目标是找到一个超平面，使得大多数数据点都落在这个超平面的一个小的距离ε内，同时尽量使得超平面的复杂度（例如，权重的大小）保持较小。

这意味着，对于训练数据中的每个数据点，只要其预测值与真实值之间的差异在ε范围内，我们就不会对其施加任何惩罚。而那些落在ε范围之外的数据点将受到相应的惩罚。这种方法允许模型在一个小的容忍范围内对数据点进行一定程度的误差。

**关键概念：**

1. **ε-insensitive loss function**：这是SVR的核心损失函数。对于每个数据点，只有当其预测值与真实值之差超过$\epsilon$时，才会产生非零的损失。这意味着，如果误差在$\epsilon$范围内，损失为0。
2. **正则化**：与支持向量分类一样，SVR也使用正则化来防止过拟合。通过引入一个正则化项，我们试图在拟合数据和保持模型简单之间找到一个平衡。

**核函数**：与SVM分类器一样，SVR也可以与各种核函数一起使用，如线性核、多项式核、径向基函数（RBF）核等，使其能够处理非线性关系。

# 6.6 核方法

书中对表示定理及其计算的具体细节都阐释的较为清楚，以下是总结得到的其他资料

## 表示定理

表示定理（Representation Theorem）是核方法与支持向量机（SVM）中的一个关键概念。它给出了在特定条件下的最优解的一种表示形式。

### 表示定理概述

在使用核方法的上下文中，表示定理说明：对于某些学习算法（如支持向量机），其最优解可以表示为所有训练样本的线性组合。换句话说，不需要知道映射到高维空间后的所有特征，我们只需要知道数据点之间在这个空间中的内积（即核函数的值）。

对于支持向量机，表示定理具体表明：最优决策函数（即分类或回归函数）可以表示为训练数据点的线性组合，其中每个数据点的权重由其对应的拉格朗日乘子给出。

### 形式化表示

对于分类问题，决策函数可以表示为：
$$
f(x)=\sum_{i=1}^m\alpha_iy_ik(x_i,x)+b
$$
其中：

- $x_i$ 是训练数据点。
- $y_i$ 是对应的类标签。
- $\alpha_i$ 是拉格朗日乘子。
- $k$ 是核函数。
- $b$ 是偏置项。

值得注意的是，对于SVM，绝大多数的拉格朗日乘子$\alpha_i$都是0，只有支持向量对应的乘子是非零的。这意味着决策函数实际上只是支持向量的线性组合，而不是所有训练数据点的。

### 重要性

表示定理对于核方法的发展和支持向量机的有效性至关重要。它允许我们在高维甚至可能是无限维的空间中有效地工作，而不需要显式地处理或计算该空间中的所有特征。这大大简化了计算，并使得非线性模型成为可能。

总的来说，表示定理为我们提供了一种框架，说明了在特定条件下，最优解的形式及如何使用核函数进行计算。这是支持向量机和其他核方法能够处理复杂数据集和非线性问题的关键原因。