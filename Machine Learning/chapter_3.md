# 第3章 线性模型

# 3.1 基本形式

机器学习中的线性模型是一种用线性方程来描述数据之间关系的模型。具体来说，给定一组输入特征，线性模型试图预测一个或多个输出，这些预测是输入特征的线性组合。

线性模型的基本形式是这样的：
$$
y=w_{1} x_{1}+w_{2} x_{2}+...+w_{n} x_{n}+b
$$
其中：

- *y* 是预测值。
- *x*1,*x*2,...,*xn* 是输入特征。
- *w*1,*w*2,...,*wn* 是模型的权重。
- *b* 是偏置项。

以下是线性模型的一些常见类型：

1. **线性回归 (Linear Regression)**: 这可能是最简单和最广泛使用的线性模型。它尝试找到输入特征与连续输出值之间的最佳线性关系。常用于预测数值型的输出。
2. **逻辑回归 (Logistic Regression)**: 尽管名字中有“回归”，但逻辑回归实际上是用于分类任务的。它预测某一类的概率，通常使用sigmoid函数将线性组合的输出映射到[0,1]范围内。
3. **线性判别分析 (Linear Discriminant Analysis, LDA)**: 这是一种分类技术，它不仅关注分类，还考虑了类别间和类别内的方差，以获得最佳的分类。
4. **感知机 (Perceptron)**: 这是一个二分类算法，它的工作原理是调整权重，直到所有数据点都被正确分类或达到最大迭代次数。

线性模型的优点包括简单、易于理解、计算效率高和可解释性强。然而，它们的局限性在于不能捕获复杂的非线性关系，所以在某些任务中可能不如其他复杂模型表现得好。



# 3.2 线性回归

## 一元线性回归

参考课程链接：【【吃瓜教程】《机器学习公式详解》（南瓜书）与西瓜书公式推导直播合集】 https://www.bilibili.com/video/BV1Mh411e7VU/?p=3&share_source=copy_web&vd_source=c22abe8e67e193936015d5ca043a8148

一元线性回归是线性回归的一种特殊情况，它只涉及一个输入特征和一个输出。其目的是找到描述输入和输出之间关系的最佳直线。一元线性回归的模型表示为：
$$
y=wx+b
$$
其中：

- *y* 是预测值
- *x* 是输入特征
- *w* 是权重（也称为斜率）
- *b* 是偏置（也称为截距）

### **算法原理**

一元线性回归的目标是最小化实际输出值 *y* 和预测输出值 *f(x)* 之间的平均平方误差（Mean Squared Error, MSE）。数学表示为：
$$
MSE=\frac{1}{m}\sum_{i=1}^{m}(f( x_{i} )-y_{i})^{2}
$$
其中 *m* 是数据点的数量。

为了最小化MSE，我们需要调整权重 *w* 和偏置 *b*。这通常通过梯度下降算法来实现。

**梯度下降**：

1. 初始化权重 *w* 和偏置 *b* 的值（通常为随机值）。
2. 使用当前的 *w* 和 *b* 计算所有数据点的预测值 *f(x)*。
3. 根据预测值计算MSE。
4. 计算MSE关于 *w* 和 *b* 的梯度。
5. 调整 *w* 和 *b* 的值，使其沿着梯度的反方向移动一个小步。
6. 重复步骤2-5，直到MSE达到一个足够小的值或满足其他终止条件。

在一元线性回归中，MSE关于 *w* 和 *b* 的梯度可以直接计算：
$$
\frac{\partial MSE}{\partial w} =\frac{2}{m}\sum_{i=1}^{m}x_{i}(f( x_{i} )-y_{i})
$$

$$
\frac{\partial MSE}{\partial b} =\frac{2}{m}\sum_{i=1}^{m}(f( x_{i} )-y_{i})
$$

梯度下降算法将使用这些梯度值来更新 *w* 和 *b*，以达到最小化MSE的目标。

注意：对于一元线性回归，还有一个解析解，称为“正规方程”或“最小二乘法”，它可以直接找到权重 *w* 和偏置 *b*，而无需迭代地使用梯度下降。

### 补充：正交回归

正交回归（也称为主轴回归或等距回归）是线性回归的一个变种，用于在两个变量都存在测量误差的情境下估计它们之间的关系。传统的线性回归（如最小二乘法）假设自变量（或解释变量）是无误差的，而只有因变量（或响应变量）存在误差。但在某些实际应用中，这个假设可能不成立。例如，在某些科学实验中，两个被测量的变量都可能受到仪器误差的影响。

正交回归的主要思想是寻找一条直线（或一个超平面，对于多维的情况），使得数据点到这条直线的垂直距离的平方和最小。

数学上，考虑两个变量X和Y。传统的最小二乘回归尝试找到一条直线使得垂直于Y轴（因变量轴）的距离的平方和最小。而正交回归则尝试找到一条直线，使得数据点到这条直线的垂直距离的平方和最小。

从几何的角度看，正交回归考虑了数据点到回归线的直线距离，而不仅仅是垂直距离。

要执行正交回归，通常需要使用迭代方法或优化技术，因为它涉及到的数学不如传统的最小二乘回归那么简单。正交回归在某些应用中很有价值，尤其是当我们有理由相信解释变量和响应变量都受到测量误差的影响时。

### 最小二乘估计与极大似然估计

线性回归的最小二乘估计（Least Squares Estimation，LSE）和极大似然估计（Maximum Likelihood Estimation，MLE）都是用来估计线性模型参数的方法。尽管它们背后的思想和方法不同，但在简单线性回归的情境下，它们实际上可以得到相同的结果。

1. **最小二乘估计 (LSE)**:

   这是线性回归中最常用的参数估计方法。其核心思想是最小化实际输出值与预测输出值之间的平方差。

   给定一个线性模型 
   $$
   y=Xβ+ϵ
   $$
   其中 *y* 是输出向量，*X* 是输入矩阵，β* 是参数向量，ϵ* 是误差项，LSE的目标是找到参数 *β* 使得 *ϵ**T**ϵ* 最小。换句话说，LSE试图最小化输出的预测值和实际值之间的总平方误差。

   对于一元线性回归，LSE可以得到一个封闭形式的解，即正规方程。

2. **极大似然估计 (MLE)**:

   MLE的目标是找到参数估计值，使得给定这些参数下，观测到实际数据的可能性（似然）最大。从统计学的角度来说，我们通常假设误差项 *ϵ* 是独立同分布的，并遵循正态分布，即 ϵ*∼*N*(0,*σ*2)。

   在这种情况下，线性回归模型的似然函数为：
   $$
   L(\beta \mid y,X)=\prod_{i=1}^{n}\frac{1}{\sqrt{2 \pi \sigma^{2} } }  exp(-\frac{(y_{i}-x_{i}^{T}\beta )^{2}}{2 \sigma^{2}} )
   $$
   对似然函数取对数，我们得到对数似然函数。然后，我们可以最大化对数似然函数来得到 *β* 的MLE。

在线性回归的简单场景下（即误差项为正态分布），LSE和MLE实际上是等价的，即它们会得到相同的参数估计值。然而，这两种方法背后的思想和解释是不同的，LSE侧重于最小化预测误差，而MLE侧重于最大化数据的似然。

### 补充：凸函数

凸函数是一种特殊类型的函数，其图形总是位于其任意两点之间的弦的上方或与之重合。更正式地说，一个函数 *f* : R*n*→R 是凸的，如果对于其定义域内的任意两点 *x* 和 *y* 以及任意 *λ* 满足 0≤*λ*≤1，以下不等式成立：
$$
f(λx+(1−λ)y)≤λf(x)+(1−λ)f(y)
$$
直观地，这意味着函数的图形在任意两点之间的线段上或以下。对于一维函数，你可以想象它为“碗”的形状，而不是“鞍”的形状。

凸函数的一个重要特性是：它的局部最小值也是全局最小值。

### 机器学习三要素

1. **模型（Model）**：

   模型定义了输入与输出之间的关系。它可以是一个简单的线性模型、一个决策树、一个神经网络或任何其他数学结构，用于描述或预测数据。模型的选择通常取决于问题的性质、数据的特征以及特定任务的需求。

2. **目标函数（Objective Function，也称为损失函数或代价函数）**：

   目标函数度量模型的预测值与实际值之间的差异。在监督学习中，目标函数常常是用来评估模型表现的关键指标，如均方误差（对于回归任务）或交叉熵损失（对于分类任务）。机器学习的核心任务之一就是优化（通常是最小化）这个函数。

3. **优化算法（Optimization Algorithm）**：

   优化算法定义了如何更新模型的参数以改进其性能，即减少目标函数的值。常见的优化算法包括梯度下降、随机梯度下降、牛顿法、L-BFGS等。这些算法通过不同的方式调整模型的参数，以寻找目标函数的最小值（或最大值，取决于问题）。

## 多元线性回归

参考课程链接：【【吃瓜教程】《机器学习公式详解》（南瓜书）与西瓜书公式推导直播合集】 https://www.bilibili.com/video/BV1Mh411e7VU/?p=4&share_source=copy_web&vd_source=c22abe8e67e193936015d5ca043a8148

### 1. 定义

多元线性回归是线性回归的扩展，用于预测一个响应变量基于两个或更多的特征。其基本假设是所有这些特征与响应变量之间存在线性关系。

### 2. 数学模型

多元线性回归的数学表示为： 
$$
y=β_{0}+β_{1}x_{1}+β_{2}x_{2}+...+β_{n}x_{n}+ϵ
$$
其中：

- *y* 是响应变量。
- *x*1,*x*2,...*xn* 是解释变量或特征。
- *β*0,*β*1,...*βn* 是模型参数，其中 *β*0 是截距。
- *ϵ* 是误差项。

### 3. 参数估计

为了确定模型的参数（*β* 值），我们通常使用**最小二乘法**来最小化预测和真实值之间的平方误差之和。

### 4. 假设

多元线性回归有以下假设：

- 线性关系：解释变量与响应变量之间存在线性关系。
- 误差的独立性：观测值的误差是独立的。
- 同方差性：误差的方差在所有观测值中是恒定的。
- 无多重共线性：解释变量之间没有完全的线性关系。
- 误差的正态性：对于任何给定的解释变量值组合，误差是正态分布的。

### 5. 诊断和检验

为了确认模型的适用性，我们需要进行一系列的诊断检查。这包括：

- **残差分析**：检查残差（实际值与预测值之间的差异）是否满足正态分布、独立性和同方差性的假设。
- **方差膨胀因子（VIF）**：检查多重共线性。
- **t-测试和F-测试**：评估模型中的单个或所有变量是否对响应变量有显著影响。

### 6. 应用

多元线性回归在许多领域都有应用，包括经济学、生物学、工程学和社会科学等。例如，预测基于多个输入特征（如面积、楼层数、社区等）的房价。

### 7. 限制和挑战

尽管多元线性回归是一个强大的工具，但它也有其限制。数据中的非线性关系、相关性高的解释变量（多重共线性）、异方差和缺乏观测独立性都可能对模型的解释和预测能力产生影响。

# 3.3 对数几率回归

参考课程链接：【【吃瓜教程】《机器学习公式详解》（南瓜书）与西瓜书公式推导直播合集】 https://www.bilibili.com/video/BV1Mh411e7VU/?p=5&share_source=copy_web&vd_source=c22abe8e67e193936015d5ca043a8148

对数几率回归，通常称为**逻辑回归**（Logistic Regression），是统计学和机器学习中的一种分类方法。尽管其名称中含有“回归”，但逻辑回归主要用于二分类问题（也可以扩展到多分类问题）。其基本思想是将线性回归的输出通过某种方法（如 Sigmoid 函数）映射到[0,1]区间，以得到某事件发生的概率。

### 1. 基本数学模型

逻辑回归使用的主要函数是 Sigmoid 函数： 
$$
σ(z)=\frac{1}{1+e^{−z}}
$$
其中，*z* 是输入值。

当我们应用线性回归模型
$$
z=β_{0}+β_{1}x_{1}+β_{2}x_{2}+...+β_{n}x_{n}
$$
 我们将 *z* 值放入 Sigmoid 函数中，得到：
$$
p=\frac{1}{1+e^{-(β_{0}+β_{1}x_{1}+β_{2}x_{2}+...+β_{n}x_{n})}}
$$
这里的 *p* 表示事件发生的概率。

### 2. 对数几率

对数几率是逻辑回归名称的来源，它是事件发生概率与事件不发生概率之比的自然对数： 
$$
ln(\frac{p}{1-p})=β_{0}+β_{1}x_{1}+β_{2}x_{2}+...+β_{n}x_{n}
$$
这里，*p* 是事件发生的概率。

### 3. 参数估计

逻辑回归的参数通常使用**最大似然估计**（MLE）来估计。

### 4. 用途

逻辑回归常用于预测一个结果是两个可能类别中的哪一个，例如：邮件是垃圾邮件还是非垃圾邮件、交易是欺诈还是合法、病患是患病还是健康等。

### 5. 优点与缺点

**优点**：

- 输出是概率得分，可解释性强。
- 计算效率高，容易实现。
- 不需要假设数据的分布，如线性回归需要假设误差服从正态分布。

**缺点**：

- 它假定特征与对数几率之间存在线性关系。
- 对于非线性关系，可能需要额外的工作如特征工程或选择其他模型。
- 容易受到无关特征和高度相关特征的影响。