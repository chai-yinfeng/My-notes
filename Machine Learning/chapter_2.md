# 2.1 经验误差与过拟合

## 概念

经验误差与泛化误差是机器学习中两个关键的概念，它们与模型的训练及其在新数据上的性能密切相关。

### 1. **经验误差 (Empirical Error)**

经验误差又被称为训练误差，是机器学习模型在训练数据集上的误差。具体来说，它是模型在训练数据上的预测与实际目标值之间的差异的平均值。

### 2. **泛化误差 (Generalization Error)**

泛化误差是模型在新、未见过的数据上的预期误差。它表示模型对新输入数据的预测与实际目标值之间的差异。泛化误差是机器学习研究的中心议题，因为它与模型在实际应用中的性能密切相关。

## **经验误差与泛化误差的关系**

理想情况下，我们希望模型在训练数据上有低的经验误差，并且在新数据上有低的泛化误差。然而，仅仅追求低的经验误差并不总是导致低的泛化误差。一个典型的例子是过拟合：

- **过拟合**：如果模型过于复杂，它可能会在训练数据上达到非常低的经验误差（甚至为零），但在新数据上的性能却很差。这是因为模型记住了训练数据中的噪声或特定细节，而这些细节在新的数据中可能并不存在。

### **追求更小的泛化误差的过程中，对经验误差的需求**

1. **权衡**：我们需要在经验误差和模型复杂度之间找到一个平衡。简单的模型可能不足以捕获数据中的所有模式（导致高的经验误差），而复杂的模型可能过拟合。
2. **验证**：为了估计泛化误差，我们经常使用验证集或交叉验证。这可以帮助我们选择那些既不是过度简化又不是过度复杂的模型。
3. **正则化**：技术如L1和L2正则化旨在防止过拟合，同时允许模型在训练数据上达到较低的经验误差。

总之，虽然经验误差是一个重要的指标，但在机器学习中，我们的主要目标是获得低的泛化误差。这可能需要牺牲一些经验误差，特别是在可能出现过拟合的情况下。

# 2.2 评估方法

测试误差是机器学习模型在测试数据集上的性能指标，通常用于评估模型的泛化能力。简单来说，测试误差给出了模型在未见过的数据上的预测误差。

- **测试误差**: 模型在测试数据集上的预测输出与实际标签之间的差异的度量。测试数据集是在模型的训练过程中没有使用的一组数据。

- **泛化性能的指标**：测试误差为我们提供了模型泛化到新数据的能力的估计。理论上，一个好的机器学习模型不仅在训练数据上表现得很好，而且在测试数据上也表现得很好。
- **比较模型**：测试误差是比较不同模型或算法性能的常用指标。模型A的测试误差低于模型B的测试误差通常意味着模型A在这特定的测试数据集上有更好的泛化性能。
- **泛化误差**：模型在所有可能的新数据上的预期误差。由于我们无法为所有可能的新样本测试模型，因此泛化误差是一个理论概念，而测试误差是其经验估计。
- **避免多次使用测试集**：为了保持测试误差的客观性，测试数据集只应在模型评估阶段使用一次。多次使用测试集可能会导致过度优化或对测试数据的过拟合。
- **代表性**：为了使测试误差成为泛化误差的有效估计，测试数据集应该具有代表性，即它应该与模型可能遇到的实际数据分布相匹配。

## 2.2.1 留出法

留出法是一种常用的模型验证技术，用于评估机器学习模型的性能。在这种方法中，我们将原始数据集分为两个或三个互斥的子集，通常为：训练集、验证集和（或）测试集。

### **留出法的步骤**

1. **划分数据**：将原始数据随机分为训练集和测试集。例如，常见的划分比例为70%的数据用于训练，30%的数据用于测试。如果还需要验证集，数据可以进一步划分，例如60%用于训练，20%用于验证，20%用于测试。
2. **训练模型**：使用训练集来训练机器学习模型。
3. **验证模型**（如果有验证集的话）：使用验证集来调整模型的超参数和其他设置，以获得最佳性能。
4. **评估模型**：使用留出的测试集来评估模型的性能。这为我们提供了模型在未见过的数据上的泛化能力的估计。

### **留出法的优点**

1. **简单快速**：相比于其他验证方法如交叉验证，留出法通常更快，因为模型只需要训练一次（或当使用验证集时训练少数次）。
2. **无偏估计**：如果测试集是随机和代表性地从原始数据中选择的，那么留出法提供的模型性能估计是无偏的。

### **留出法的缺点**

1. **高方差**：由于模型的性能评估仅基于一个数据子集，结果可能因数据的随机划分方式而有所不同。
2. **利用率不高**：部分数据（如测试集）在模型训练过程中未被使用，这可能导致模型未充分利用所有可用数据。
3. **数据分布问题**：如果数据没有正确地或均匀地划分，例如某些类别的样本在测试集中过多或过少，那么性能评估可能会有偏。

## 2.2.2 交叉验证法

交叉验证法（Cross-Validation，简称CV）是一种用于评估机器学习模型泛化性能的统计技术。它减少了因数据随机划分引起的性能评估变异性，从而提供了模型性能的更稳定和可靠的估计。

最常见的形式是**k折交叉验证**，具体步骤如下：

1. **数据划分**：将数据集随机分为k个互斥的子集（或“折”），每个子集大致具有相同的大小。
2. **模型训练与评估**：
   - 对于每个子集，将其作为测试集，而其他k-1个子集作为训练集。
   - 在训练集上训练模型。
   - 在当前的测试集上评估模型。
   - 记录当前的测试误差。
3. **平均误差**：重复第2步k次，每次选择不同的子集作为测试集。最后，取k次测试误差的平均值作为模型的最终评估指标。

### **交叉验证的优点**

- **更稳定的性能估计**：通过在多个数据子集上训练和评估模型，交叉验证减少了性能估计的变异性。
- **更充分地利用数据**：每个数据点都被用作训练和测试，这与留出法不同，其中某些数据点可能只被用于测试。

### **交叉验证的缺点**

- **计算开销**：交叉验证需要训练k个模型，这可能在大数据集或计算密集型模型中变得非常耗时。
- **数据不平衡**：在存在数据不平衡的情况下，每一折中正负样本的比例需要仔细考虑，以确保每一折的数据分布相似。为此，可以使用分层k折交叉验证（Stratified k-Fold Cross-Validation），确保每一折中各类别的样本比例与整体数据集相同。

尽管存在上述缺点，但交叉验证仍然是评估模型泛化能力的黄金标准，尤其是当数据量有限时。

## 2.2.3 自助法

自助法（Bootstrap）是一种从原始数据集中进行有放回抽样的统计方法，用于估计统计量的分布。在机器学习中，自助法常被用于评估模型的性能，特别是当数据集很小且无法轻易地进行传统的数据划分时。

自助法在模型验证中的应用步骤如下：

1. **有放回抽样**：从原始数据集（包含N个样本）中随机选择一个样本，并将其加入新的数据集，然后将样本放回原始数据集。重复这个过程N次，从而得到一个新的大小为N的数据集。由于是有放回的抽样，新数据集中某些样本可能会出现多次，而某些原始数据集中的样本可能不会出现。
2. **模型训练与评估**：
   - 使用上述步骤生成的数据集训练模型。
   - 使用那些在自助抽样中没有被选中的样本（称为"袋外"数据，Out-of-Bag，简称OOB）来评估模型的性能。
3. **重复过程**：重复上述两个步骤多次，从而得到模型性能的多个估计值。
4. **计算平均性能**：将多次迭代中的模型性能指标取平均，得到模型的最终性能估计。

### **自助法的优点**

- **充分利用数据**：当数据集很小时，自助法能最大化地利用数据进行模型训练。
- **提供误差估计**：多次迭代生成的模型性能估计可以用于计算误差的方差和置信区间。

### **自助法的缺点**

- **偏差**：由于某些样本在训练集中可能出现多次，而某些样本可能不出现，这可能导致模型的估计有偏差。
- **不适用于所有数据**：对于时间序列数据或具有特定结构的数据，自助法可能不是一个好的选择，因为随机抽样可能会破坏数据的内在结构。

## 2.2.4 调参与最终模型

调参（参数调优）是机器学习模型开发过程中的一个关键步骤。其目的是找到最佳的参数组合，使模型在未见过的数据上表现得尽可能好。以下是调参过程的常见步骤和一些建议：

1. **理解模型和参数**：在开始调参之前，需要对模型和其参数有深入的理解。了解每个参数的意义、范围和默认值是很重要的，因为这可以帮助你确定调优的起始点。
2. **定义评估指标**：确定如何衡量模型的性能。例如，对于分类任务，常用的评估指标包括精确度、召回率、F1得分、AUC等。
3. **分割数据**：确保你有一个单独的验证集或使用交叉验证来评估模型的性能。这有助于确保你的调参过程不会导致对训练数据的过拟合。
4. **采用基准模型**：在进行任何调参之前，使用模型的默认参数运行一个基准模型。这为你提供了一个性能基准，以便与后续的优化进行比较。
5. **手动调参**：开始时，你可以手动调整一两个关键参数来获取对参数如何影响模型性能的直观感受。
6. **网格搜索**：这是一种系统地探索参数组合的方法。为每个参数指定一个值范围，然后尝试所有可能的组合。这种方法可能很耗时，但它可以找到参数空间内的最佳组合。
7. **随机搜索**：与网格搜索相反，随机搜索是从参数的分布中随机选择值，而不是尝试所有可能的值。这种方法通常比网格搜索更快，并且在很多情况下都能找到同样好或更好的参数组合。
8. **贝叶斯优化**：这是一种更高级的参数搜索技术，它使用概率模型预测哪些参数组合可能会产生更好的结果，并据此选择新的参数。
9. **迭代和细化**：当你找到一个好的参数组合后，可以在该组合附近进行进一步的搜索，以微调模型性能。
10. **正则化和集成**：除了模型的主要参数外，还可以考虑如何使用正则化技术（如L1或L2正则化）防止过拟合，或使用集成方法（如bagging或boosting）提高模型性能。
11. **验证和测试**：一旦确定了参数，使用验证集评估模型的性能。当你对模型满意时，使用测试集进行最终的评估。

总之，调参是一个迭代和试验的过程。在此过程中，经验、直觉和系统的搜索方法结合使用往往能产生最佳结果。

# 2.3 性能度量

### 均方误差 MSE（mean squared error）

$$
E(f;D)=\frac{1}{m}\sum_{i=1}^{m}(f( x_{i} )-y_{i})^{2}
$$

$$
E(f;\mathcal{D})=\int_{x\sim \mathcal{D} }^{}(f( x )-y)^{2} p(x)\mathrm{d}x
$$



## 2.3.1 错误率与精度

### 分类错误率

$$
E(f;D)=\frac{1}{m}\sum_{i=1}^{m}\mathbb{I}(f( x_{i} ) \neq{} y_{i})
$$

$$
E(f;\mathcal{D})=\int_{x\sim \mathcal{D} }^{}\mathbb{I}(f( x ) \neq{}y) p(x)\mathrm{d}x
$$



### 精度

$$
acc(f;D)=\frac{1}{m}\sum_{i=1}^{m}\mathbb{I}(f( x_{i} ) = y_{i})=1-E(f;D)
$$

$$
acc(f;\mathcal{D})=\int_{x\sim \mathcal{D} }^{}\mathbb{I}(f( x ) =y) p(x)\mathrm{d}x=1-E(f;\mathcal{D})
$$



## 2.3.2 查准率、查全率与F1

|              |   预测结果   |   预测结果   |
| :----------: | :----------: | :----------: |
| **真实情况** |     正例     |     反例     |
|     正例     | TP（真正例） | FN（假反例） |
|     反例     | FP（假正例） | TN（真反例） |

查准率P和查全率R
$$
P=\frac{TP}{TP+FP}
$$

$$
R=\frac{TP}{TP+FN}
$$

F1 Score
$$
F1=\frac{2*P*R}{P+R}=\frac{2*TP}{样例总数+TP-TN}
$$
在某些应用中，查准率可能比查全率更为重要，反之亦然。例如，在垃圾邮件过滤中，我们可能更关心查准率，因为我们不希望误判正常邮件为垃圾邮件。而在疾病检测中，查全率可能更重要，因为我们希望确保尽可能多的病例被检测出来，即使这意味着有一些误报。

当在特定应用中需要同时关心查准率和查全率时，F1值可以为我们提供一个平衡这两个指标的方法。

## 2.3.3 ROC与AUC

ROC（Receiver Operating Characteristic）曲线是用于评估分类模型性能的一个工具，特别是在不同的决策阈值下的性能。它最初是在二战期间用于电子信号检测理论中开发的，但后来被广泛应用于医学和机器学习领域来评估二分类问题的性能。

ROC曲线的主要组成部分如下：

1. **横轴（False Positive Rate，FPR）**:

   - FPR 表示被错误分类为正类的负类样本所占的比例。

   - 计算公式：
     $$
     FPR=\frac{FP}{TN+FP}
     $$
     

   - 其中，FP 是假正类数量，TN 是真负类数量。

2. **纵轴（True Positive Rate，TPR，也被称为召回率或查全率）**:

   - TPR 表示被正确分类为正类的正类样本所占的比例。

   - 计算公式：
     $$
     TPR=\frac{TP}{TP+FN}
     $$
     

   - 其中，TP 是真正类数量，FN 是假负类数量。

ROC 曲线是通过改变分类决策的阈值来画出的，从而绘制出各种可能的 FPR 和 TPR 组合。

**AUC（Area Under the Curve）** 是ROC曲线下的面积，它提供了一个衡量分类器整体性能的标量值。AUC的取值范围为0到1，值越大表示模型的性能越好。一个完美的分类器的 AUC 为 1，而一个随机猜测的分类器的 AUC 为 0.5。

在实际应用中，ROC 和 AUC 被广泛用于：

- 比较不同模型的性能。
- 选择最佳的决策阈值，特别是当正负类的代价不同等时。

### 补充

TPR（True Positive Rate）和Recall（R，又称为查全率或召回率）的计算公式是完全一样的。那么，为什么我们需要两个不同的术语来描述同一个量呢？

这两个术语的存在和应用背景有关：

1. **应用上下文**：
   - **Recall（召回率/查全率）**：这个术语主要在机器学习和统计学中使用，用于描述一个分类器能正确识别出的正类实例的比例。召回率通常与查准率（Precision）一起使用，这两者经常在模型性能评估中被提及，特别是在信息检索和推荐系统中。
   - **TPR（True Positive Rate）**：这个术语主要在ROC曲线的上下文中使用。在ROC曲线中，我们比较TPR和FPR（False Positive Rate）来评估不同决策阈值下分类器的性能。
2. **讨论的重点**：
   - 当我们谈论**召回率**时，我们的重点通常是分类器如何处理正类样本，特别是当正类是我们关心的少数类（例如，医学检测中的疾病存在情况）。
   - 当我们谈论**TPR**时，我们的重点是它与FPR的关系，以及如何在整个ROC曲线上评估模型。

尽管它们在定义和计算上是一样的，但由于应用背景和重点的不同，它们在不同的上下文中使用不同的名称。这可以帮助读者和听众更清晰地理解正在讨论的主题和关键指标。

## 2.3.4 代价敏感错误率与代价曲线

### 代价敏感错误率

代价敏感错误率（Cost-sensitive Error Rate）是一个机器学习的概念，它基于一个简单的事实：不同类型的错误可能有不同的代价。在许多实际应用中，某些错误的代价可能远高于其他错误，因此需要对不同的错误赋予不同的权重或代价。

在传统的分类问题中，我们通常假设所有的错误都具有相同的代价。然而，代价敏感学习的目的是为了最小化总的错误代价，而不仅仅是错误的数量。

例如，考虑医疗诊断的情境：

- 假设病人实际患有某种疾病，但被误诊为健康，这种类型的错误可能会导致病情恶化，甚至死亡，所以代价很高。
- 另一方面，如果一个健康的病人被误诊为患有疾病，尽管这是不好的，但可能只会导致不必要的进一步检查和焦虑，其代价相对较低。

为了捕捉这种不平衡的代价，我们可以为每种错误类型分配一个代价值或权重，并在模型评估和训练中使用它们。

具体实现时，通常会使用一个代价矩阵，如下：

|            | 预测为正类 | 预测为负类 |
| :--------: | :--------: | :--------: |
| 实际为正类 |   C(TP)    |   C(FN)    |
| 实际为负类 |   C(FP)    |   C(TN)    |

其中：

- C(TP) 和 C(TN) 通常为0，因为正确的分类没有“代价”。
- C(FP) 和 C(FN) 是两种类型错误的代价，它们可能不相等。

模型的目标是最小化总体代价，这可能意味着愿意接受更多的低代价错误以避免高代价错误。

代价敏感错误率的思想可应用于模型训练、模型评估和决策阈值的选择，以确保机器学习解决方案在实际应用中具有最大的实际价值和效益。

### 代价曲线

代价曲线（Cost Curve）是一个可视化工具，用于评估和比较不同的分类模型在考虑错误代价时的性能。代价曲线基于代价敏感错误率的概念，因此能够反映出不同类型的错误所带来的不同代价。

代价曲线与ROC曲线相似，但它不是展示TPR和FPR，而是展示与预测阈值相关的预期错误代价。

以下是代价曲线的主要构建步骤：

1. **定义代价矩阵**：为各种错误类型（如FN和FP）设定具体的代价值。
2. **计算预期代价**：对于每一个可能的预测阈值，使用代价矩阵计算预期的总代价。
3. **绘制曲线**：x轴为不同的预测阈值，y轴为对应的预期代价。曲线展示了预测阈值变化时，预期代价如何变化。

代价曲线的优势：

- **直观**：代价曲线直观地展示了不同决策阈值下模型的预期代价，从而帮助我们选择最佳的决策阈值。
- **代价敏感**：与ROC曲线不同，代价曲线明确地考虑了错误的代价，更适合于那些错误代价不平衡的应用场景。
- **比较模型**：可以用代价曲线比较不同模型的性能，选择预期代价最低的模型。

实际应用中，代价曲线尤其适用于那些不同类型的错误代价明显不同的场景，如医疗诊断、金融欺诈检测等。在这些场景中，选择适当的决策阈值并考虑错误的实际代价是至关重要的。

# 2.4 比较检验



## 2.4.1 假设检验



## 2.4.2 交叉验证t检验



## 2.4.3 McNemar检验



## 2.4.4 Friedman检验 与Nemenyi后续检验



# 2.5 偏差与方差