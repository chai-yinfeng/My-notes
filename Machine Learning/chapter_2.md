# 2.1 经验误差与过拟合

## 概念

经验误差与泛化误差是机器学习中两个关键的概念，它们与模型的训练及其在新数据上的性能密切相关。

### 1. **经验误差 (Empirical Error)**

经验误差又被称为训练误差，是机器学习模型在训练数据集上的误差。具体来说，它是模型在训练数据上的预测与实际目标值之间的差异的平均值。

### 2. **泛化误差 (Generalization Error)**

泛化误差是模型在新、未见过的数据上的预期误差。它表示模型对新输入数据的预测与实际目标值之间的差异。泛化误差是机器学习研究的中心议题，因为它与模型在实际应用中的性能密切相关。

## **经验误差与泛化误差的关系**

理想情况下，我们希望模型在训练数据上有低的经验误差，并且在新数据上有低的泛化误差。然而，仅仅追求低的经验误差并不总是导致低的泛化误差。一个典型的例子是过拟合：

- **过拟合**：如果模型过于复杂，它可能会在训练数据上达到非常低的经验误差（甚至为零），但在新数据上的性能却很差。这是因为模型记住了训练数据中的噪声或特定细节，而这些细节在新的数据中可能并不存在。

### **追求更小的泛化误差的过程中，对经验误差的需求**

1. **权衡**：我们需要在经验误差和模型复杂度之间找到一个平衡。简单的模型可能不足以捕获数据中的所有模式（导致高的经验误差），而复杂的模型可能过拟合。
2. **验证**：为了估计泛化误差，我们经常使用验证集或交叉验证。这可以帮助我们选择那些既不是过度简化又不是过度复杂的模型。
3. **正则化**：技术如L1和L2正则化旨在防止过拟合，同时允许模型在训练数据上达到较低的经验误差。

总之，虽然经验误差是一个重要的指标，但在机器学习中，我们的主要目标是获得低的泛化误差。这可能需要牺牲一些经验误差，特别是在可能出现过拟合的情况下。

# 2.2 评估方法

测试误差是机器学习模型在测试数据集上的性能指标，通常用于评估模型的泛化能力。简单来说，测试误差给出了模型在未见过的数据上的预测误差。

- **测试误差**: 模型在测试数据集上的预测输出与实际标签之间的差异的度量。测试数据集是在模型的训练过程中没有使用的一组数据。

- **泛化性能的指标**：测试误差为我们提供了模型泛化到新数据的能力的估计。理论上，一个好的机器学习模型不仅在训练数据上表现得很好，而且在测试数据上也表现得很好。
- **比较模型**：测试误差是比较不同模型或算法性能的常用指标。模型A的测试误差低于模型B的测试误差通常意味着模型A在这特定的测试数据集上有更好的泛化性能。
- **泛化误差**：模型在所有可能的新数据上的预期误差。由于我们无法为所有可能的新样本测试模型，因此泛化误差是一个理论概念，而测试误差是其经验估计。
- **避免多次使用测试集**：为了保持测试误差的客观性，测试数据集只应在模型评估阶段使用一次。多次使用测试集可能会导致过度优化或对测试数据的过拟合。
- **代表性**：为了使测试误差成为泛化误差的有效估计，测试数据集应该具有代表性，即它应该与模型可能遇到的实际数据分布相匹配。

## 2.2.1 留出法

留出法是一种常用的模型验证技术，用于评估机器学习模型的性能。在这种方法中，我们将原始数据集分为两个或三个互斥的子集，通常为：训练集、验证集和（或）测试集。

### **留出法的步骤**

1. **划分数据**：将原始数据随机分为训练集和测试集。例如，常见的划分比例为70%的数据用于训练，30%的数据用于测试。如果还需要验证集，数据可以进一步划分，例如60%用于训练，20%用于验证，20%用于测试。
2. **训练模型**：使用训练集来训练机器学习模型。
3. **验证模型**（如果有验证集的话）：使用验证集来调整模型的超参数和其他设置，以获得最佳性能。
4. **评估模型**：使用留出的测试集来评估模型的性能。这为我们提供了模型在未见过的数据上的泛化能力的估计。

### **留出法的优点**

1. **简单快速**：相比于其他验证方法如交叉验证，留出法通常更快，因为模型只需要训练一次（或当使用验证集时训练少数次）。
2. **无偏估计**：如果测试集是随机和代表性地从原始数据中选择的，那么留出法提供的模型性能估计是无偏的。

### **留出法的缺点**

1. **高方差**：由于模型的性能评估仅基于一个数据子集，结果可能因数据的随机划分方式而有所不同。
2. **利用率不高**：部分数据（如测试集）在模型训练过程中未被使用，这可能导致模型未充分利用所有可用数据。
3. **数据分布问题**：如果数据没有正确地或均匀地划分，例如某些类别的样本在测试集中过多或过少，那么性能评估可能会有偏。

## 2.2.2 交叉验证法

交叉验证法（Cross-Validation，简称CV）是一种用于评估机器学习模型泛化性能的统计技术。它减少了因数据随机划分引起的性能评估变异性，从而提供了模型性能的更稳定和可靠的估计。

最常见的形式是**k折交叉验证**，具体步骤如下：

1. **数据划分**：将数据集随机分为k个互斥的子集（或“折”），每个子集大致具有相同的大小。
2. **模型训练与评估**：
   - 对于每个子集，将其作为测试集，而其他k-1个子集作为训练集。
   - 在训练集上训练模型。
   - 在当前的测试集上评估模型。
   - 记录当前的测试误差。
3. **平均误差**：重复第2步k次，每次选择不同的子集作为测试集。最后，取k次测试误差的平均值作为模型的最终评估指标。

### **交叉验证的优点**

- **更稳定的性能估计**：通过在多个数据子集上训练和评估模型，交叉验证减少了性能估计的变异性。
- **更充分地利用数据**：每个数据点都被用作训练和测试，这与留出法不同，其中某些数据点可能只被用于测试。

### **交叉验证的缺点**

- **计算开销**：交叉验证需要训练k个模型，这可能在大数据集或计算密集型模型中变得非常耗时。
- **数据不平衡**：在存在数据不平衡的情况下，每一折中正负样本的比例需要仔细考虑，以确保每一折的数据分布相似。为此，可以使用分层k折交叉验证（Stratified k-Fold Cross-Validation），确保每一折中各类别的样本比例与整体数据集相同。

尽管存在上述缺点，但交叉验证仍然是评估模型泛化能力的黄金标准，尤其是当数据量有限时。

## 2.2.3 自助法

自助法（Bootstrap）是一种从原始数据集中进行有放回抽样的统计方法，用于估计统计量的分布。在机器学习中，自助法常被用于评估模型的性能，特别是当数据集很小且无法轻易地进行传统的数据划分时。

自助法在模型验证中的应用步骤如下：

1. **有放回抽样**：从原始数据集（包含N个样本）中随机选择一个样本，并将其加入新的数据集，然后将样本放回原始数据集。重复这个过程N次，从而得到一个新的大小为N的数据集。由于是有放回的抽样，新数据集中某些样本可能会出现多次，而某些原始数据集中的样本可能不会出现。
2. **模型训练与评估**：
   - 使用上述步骤生成的数据集训练模型。
   - 使用那些在自助抽样中没有被选中的样本（称为"袋外"数据，Out-of-Bag，简称OOB）来评估模型的性能。
3. **重复过程**：重复上述两个步骤多次，从而得到模型性能的多个估计值。
4. **计算平均性能**：将多次迭代中的模型性能指标取平均，得到模型的最终性能估计。

### **自助法的优点**

- **充分利用数据**：当数据集很小时，自助法能最大化地利用数据进行模型训练。
- **提供误差估计**：多次迭代生成的模型性能估计可以用于计算误差的方差和置信区间。

### **自助法的缺点**

- **偏差**：由于某些样本在训练集中可能出现多次，而某些样本可能不出现，这可能导致模型的估计有偏差。
- **不适用于所有数据**：对于时间序列数据或具有特定结构的数据，自助法可能不是一个好的选择，因为随机抽样可能会破坏数据的内在结构。

## 2.2.4 调参与最终模型

调参（参数调优）是机器学习模型开发过程中的一个关键步骤。其目的是找到最佳的参数组合，使模型在未见过的数据上表现得尽可能好。以下是调参过程的常见步骤和一些建议：

1. **理解模型和参数**：在开始调参之前，需要对模型和其参数有深入的理解。了解每个参数的意义、范围和默认值是很重要的，因为这可以帮助你确定调优的起始点。
2. **定义评估指标**：确定如何衡量模型的性能。例如，对于分类任务，常用的评估指标包括精确度、召回率、F1得分、AUC等。
3. **分割数据**：确保你有一个单独的验证集或使用交叉验证来评估模型的性能。这有助于确保你的调参过程不会导致对训练数据的过拟合。
4. **采用基准模型**：在进行任何调参之前，使用模型的默认参数运行一个基准模型。这为你提供了一个性能基准，以便与后续的优化进行比较。
5. **手动调参**：开始时，你可以手动调整一两个关键参数来获取对参数如何影响模型性能的直观感受。
6. **网格搜索**：这是一种系统地探索参数组合的方法。为每个参数指定一个值范围，然后尝试所有可能的组合。这种方法可能很耗时，但它可以找到参数空间内的最佳组合。
7. **随机搜索**：与网格搜索相反，随机搜索是从参数的分布中随机选择值，而不是尝试所有可能的值。这种方法通常比网格搜索更快，并且在很多情况下都能找到同样好或更好的参数组合。
8. **贝叶斯优化**：这是一种更高级的参数搜索技术，它使用概率模型预测哪些参数组合可能会产生更好的结果，并据此选择新的参数。
9. **迭代和细化**：当你找到一个好的参数组合后，可以在该组合附近进行进一步的搜索，以微调模型性能。
10. **正则化和集成**：除了模型的主要参数外，还可以考虑如何使用正则化技术（如L1或L2正则化）防止过拟合，或使用集成方法（如bagging或boosting）提高模型性能。
11. **验证和测试**：一旦确定了参数，使用验证集评估模型的性能。当你对模型满意时，使用测试集进行最终的评估。

总之，调参是一个迭代和试验的过程。在此过程中，经验、直觉和系统的搜索方法结合使用往往能产生最佳结果。

# 2.3 性能度量

### 均方误差 MSE（mean squared error）

$$
E(f;D)=\frac{1}{m}\sum_{i=1}^{m}(f( x_{i} )-y_{i})^{2}
$$

$$
E(f;\mathcal{D})=\int_{x\sim \mathcal{D} }^{}(f( x )-y)^{2} p(x)\mathrm{d}x
$$



## 2.3.1 错误率与精度

### 分类错误率

$$
E(f;D)=\frac{1}{m}\sum_{i=1}^{m}\mathbb{I}(f( x_{i} ) \neq{} y_{i})
$$

$$
E(f;\mathcal{D})=\int_{x\sim \mathcal{D} }^{}\mathbb{I}(f( x ) \neq{}y) p(x)\mathrm{d}x
$$



### 精度

$$
acc(f;D)=\frac{1}{m}\sum_{i=1}^{m}\mathbb{I}(f( x_{i} ) = y_{i})=1-E(f;D)
$$

$$
acc(f;\mathcal{D})=\int_{x\sim \mathcal{D} }^{}\mathbb{I}(f( x ) =y) p(x)\mathrm{d}x=1-E(f;\mathcal{D})
$$



## 2.3.2 查准率、查全率与F1

|              |   预测结果   |   预测结果   |
| :----------: | :----------: | :----------: |
| **真实情况** |     正例     |     反例     |
|     正例     | TP（真正例） | FN（假反例） |
|     反例     | FP（假正例） | TN（真反例） |

查准率P和查全率R
$$
P=\frac{TP}{TP+FP}
$$

$$
R=\frac{TP}{TP+FN}
$$



## 2.3.3 ROC与AUC



## 2.3.4 代价敏感错误率与代价曲线



# 2.4 比较检验



## 2.4.1 假设检验



## 2.4.2 交叉验证t检验



## 2.4.3 McNemar检验



## 2.4.4 Friedman检验 与Nemenyi后续检验



# 2.5 偏差与方差